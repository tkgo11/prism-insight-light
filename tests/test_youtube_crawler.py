#!/usr/bin/env python3
"""
Test script for YouTube Event Fund Crawler

Quick validation of individual components without full workflow execution.
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from youtube_event_fund_crawler import YouTubeEventFundCrawler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_rss_fetch():
    """Test RSS feed fetching"""
    logger.info("="*80)
    logger.info("TEST 1: RSS Feed Fetching")
    logger.info("="*80)

    try:
        crawler = YouTubeEventFundCrawler()
        videos = crawler.fetch_latest_videos()

        if videos:
            logger.info(f"‚úÖ Successfully fetched {len(videos)} videos")
            logger.info("\nLatest video:")
            logger.info(f"  - Title: {videos[0]['title']}")
            logger.info(f"  - ID: {videos[0]['id']}")
            logger.info(f"  - Published: {videos[0]['published']}")
            logger.info(f"  - Link: {videos[0]['link']}")
            return True
        else:
            logger.error("‚ùå No videos found")
            return False

    except Exception as e:
        logger.error(f"‚ùå RSS fetch failed: {e}", exc_info=True)
        return False


def test_video_history():
    """Test video history save/load"""
    logger.info("\n" + "="*80)
    logger.info("TEST 2: Video History Management")
    logger.info("="*80)

    try:
        crawler = YouTubeEventFundCrawler()

        # Create test data
        test_videos = [
            {
                'id': 'test123',
                'title': 'Test Video',
                'published': '2025-11-22T00:00:00Z',
                'link': 'https://youtube.com/watch?v=test123',
                'author': 'Test Author'
            }
        ]

        # Test save
        crawler.save_video_history(test_videos)
        logger.info("‚úÖ Saved test video history")

        # Test load
        loaded_videos = crawler.load_previous_videos()
        if loaded_videos and loaded_videos[0]['id'] == 'test123':
            logger.info("‚úÖ Loaded video history successfully")
            logger.info(f"  - Loaded {len(loaded_videos)} videos")
            return True
        else:
            logger.error("‚ùå Video history mismatch")
            return False

    except Exception as e:
        logger.error(f"‚ùå Video history test failed: {e}", exc_info=True)
        return False


def test_new_video_detection():
    """Test new video detection logic"""
    logger.info("\n" + "="*80)
    logger.info("TEST 3: New Video Detection")
    logger.info("="*80)

    try:
        crawler = YouTubeEventFundCrawler()

        # Create mock data
        previous_videos = [
            {'id': 'old1', 'title': 'Old Video 1', 'published': '2025-11-20', 'link': 'http://...'},
            {'id': 'old2', 'title': 'Old Video 2', 'published': '2025-11-19', 'link': 'http://...'}
        ]

        current_videos = [
            {'id': 'new1', 'title': 'New Video 1', 'published': '2025-11-22', 'link': 'http://...'},
            {'id': 'old1', 'title': 'Old Video 1', 'published': '2025-11-20', 'link': 'http://...'},
            {'id': 'old2', 'title': 'Old Video 2', 'published': '2025-11-19', 'link': 'http://...'}
        ]

        # Test detection
        new_videos = crawler.find_new_videos(current_videos, previous_videos)

        if len(new_videos) == 1 and new_videos[0]['id'] == 'new1':
            logger.info("‚úÖ New video detection successful")
            logger.info(f"  - Found {len(new_videos)} new video(s)")
            logger.info(f"  - New video: {new_videos[0]['title']}")
            return True
        else:
            logger.error(f"‚ùå Detection failed: found {len(new_videos)} videos (expected 1)")
            return False

    except Exception as e:
        logger.error(f"‚ùå New video detection failed: {e}", exc_info=True)
        return False


def test_agent_creation():
    """Test AI agent creation (no execution)"""
    logger.info("\n" + "="*80)
    logger.info("TEST 4: AI Agent Creation")
    logger.info("="*80)

    try:
        crawler = YouTubeEventFundCrawler()

        video_info = {
            'title': 'Test Video',
            'published': '2025-11-22',
            'link': 'https://youtube.com/watch?v=test'
        }

        transcript = "Ïù¥Í≤ÉÏùÄ ÌÖåÏä§Ìä∏ ÏûêÎßâÏûÖÎãàÎã§. ÏãúÏû•Ïù¥ ÏÉÅÏäπÌï† Í≤ÉÏúºÎ°ú Î≥¥ÏûÖÎãàÎã§."

        agent = crawler.create_analysis_agent(video_info, transcript)

        if agent and hasattr(agent, 'instruction'):
            logger.info("‚úÖ Agent created successfully")
            logger.info(f"  - Agent name: {agent.name}")
            logger.info(f"  - Instruction length: {len(agent.instruction)} chars")
            return True
        else:
            logger.error("‚ùå Agent creation failed")
            return False

    except Exception as e:
        logger.error(f"‚ùå Agent creation failed: {e}", exc_info=True)
        return False


async def test_analysis_mock():
    """Test analysis with mock transcript (no actual video download)"""
    logger.info("\n" + "="*80)
    logger.info("TEST 5: Analysis Execution (Mock)")
    logger.info("="*80)

    try:
        crawler = YouTubeEventFundCrawler()

        video_info = {
            'title': 'üìà ÏΩîÏä§Ìîº 3,000 ÎèåÌåå ÏûÑÎ∞ï! ÏßÄÍ∏àÏù¥ Îß§Ïàò Ï†ÅÍ∏∞',
            'published': '2025-11-22T09:00:00Z',
            'link': 'https://youtube.com/watch?v=test123'
        }

        # Mock transcript
        transcript = """
        ÏïàÎÖïÌïòÏÑ∏Ïöî, Ï†ÑÏù∏Íµ¨ÏûÖÎãàÎã§. Ïò§ÎäòÏùÄ ÏΩîÏä§Ìîº Ï†ÑÎßùÏóê ÎåÄÌï¥ ÎßêÏîÄÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.

        ÏµúÍ∑º ÏãúÏû• ÏÉÅÌô©ÏùÑ Î≥¥Î©¥ ÏÉÅÏäπ Î™®Î©òÌÖÄÏù¥ Îß§Ïö∞ Í∞ïÌï©ÎãàÎã§. Ïô∏Íµ≠Ïù∏ Îß§ÏàòÏÑ∏Í∞Ä Ïù¥Ïñ¥ÏßÄÍ≥† ÏûàÍ≥†,
        Î∞òÎèÑÏ≤¥ ÏóÖÏ¢ÖÏùò Ïã§Ï†ÅÏù¥ Í∞úÏÑ†ÎêòÎ©¥ÏÑú ÏΩîÏä§ÌîºÍ∞Ä 3,000ÏÑ†ÏùÑ ÎèåÌååÌï† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏïÑ Î≥¥ÏûÖÎãàÎã§.

        ÏßÄÍ∏àÏùÄ Îß§Ïàò Ï†ÅÍ∏∞ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. ÌäπÌûà ÏÇºÏÑ±Ï†ÑÏûêÏôÄ SKÌïòÏù¥ÎãâÏä§ Í∞ôÏùÄ ÎåÄÌòïÏ£ºÏóê
        Ìà¨ÏûêÌïòÏãúÎäî Í≤ÉÏùÑ Ï∂îÏ≤úÎìúÎ¶ΩÎãàÎã§.

        Îã®Í∏∞Ï†ÅÏúºÎ°ú ÏùºÎ∂Ä Ï°∞Ï†ïÏù¥ ÏûàÏùÑ Ïàò ÏûàÏßÄÎßå, Ï§ëÏû•Í∏∞Ï†ÅÏúºÎ°ú ÏÉÅÏäπ Ï∂îÏÑ∏Îäî Í≥ÑÏÜçÎê† Í≤ÉÏúºÎ°ú
        ÏòàÏÉÅÎê©ÎãàÎã§. ÌòÑÍ∏à ÎπÑÏ§ëÏùÑ ÎÇÆÏ∂îÍ≥† Ï£ºÏãù ÎπÑÏ§ëÏùÑ ÎÜíÏù¥Îäî Í≤ÉÏù¥ Ï¢ãÍ≤†ÏäµÎãàÎã§.
        """

        logger.info("Running mock analysis (this may take 30-60 seconds)...")
        analysis = await crawler.analyze_video(video_info, transcript)

        if analysis and len(analysis) > 100:
            logger.info("‚úÖ Analysis completed successfully")
            logger.info(f"  - Analysis length: {len(analysis)} chars")
            logger.info("\n--- ANALYSIS PREVIEW (first 500 chars) ---")
            logger.info(analysis[:500] + "...")
            logger.info("--- END PREVIEW ---")
            return True
        else:
            logger.error("‚ùå Analysis failed or too short")
            return False

    except Exception as e:
        logger.error(f"‚ùå Analysis execution failed: {e}", exc_info=True)
        return False


async def main():
    """Run all tests"""
    logger.info("\n" + "üß™ "*40)
    logger.info("YouTube Event Fund Crawler - Test Suite")
    logger.info("üß™ "*40 + "\n")

    # Check mcp_agent.secrets.yaml exists
    secrets_file = Path(__file__).parent.parent / "mcp_agent.secrets.yaml"
    if not secrets_file.exists():
        logger.error("‚ùå mcp_agent.secrets.yaml not found")
        logger.error("Please copy mcp_agent.secrets.yaml.example and configure your API keys")
        return

    import yaml
    try:
        with open(secrets_file, 'r') as f:
            secrets = yaml.safe_load(f)
        openai_api_key = secrets.get('openai', {}).get('api_key')
        if not openai_api_key or openai_api_key == "example key":
            logger.error("‚ùå OPENAI_API_KEY not configured in mcp_agent.secrets.yaml")
            logger.error("Please set openai.api_key in the secrets file")
            return
    except Exception as e:
        logger.error(f"‚ùå Error loading secrets file: {e}")
        return

    results = []

    # Run tests
    results.append(("RSS Fetch", test_rss_fetch()))
    results.append(("Video History", test_video_history()))
    results.append(("New Video Detection", test_new_video_detection()))
    results.append(("Agent Creation", test_agent_creation()))

    # Analysis test (async, requires API call)
    logger.info("\n‚ö†Ô∏è  The next test will make an actual OpenAI API call")
    logger.info("‚ö†Ô∏è  This will consume API credits")

    user_input = input("\nProceed with analysis test? (y/n): ").strip().lower()
    if user_input == 'y':
        analysis_result = await test_analysis_mock()
        results.append(("Analysis Execution", analysis_result))
    else:
        logger.info("Skipping analysis test")
        results.append(("Analysis Execution", None))

    # Summary
    logger.info("\n" + "="*80)
    logger.info("TEST SUMMARY")
    logger.info("="*80)

    for test_name, result in results:
        if result is True:
            status = "‚úÖ PASS"
        elif result is False:
            status = "‚ùå FAIL"
        else:
            status = "‚äò SKIP"

        logger.info(f"{test_name:.<50} {status}")

    passed = sum(1 for _, r in results if r is True)
    failed = sum(1 for _, r in results if r is False)
    skipped = sum(1 for _, r in results if r is None)

    logger.info("="*80)
    logger.info(f"Total: {len(results)} | Passed: {passed} | Failed: {failed} | Skipped: {skipped}")
    logger.info("="*80)


if __name__ == "__main__":
    asyncio.run(main())
